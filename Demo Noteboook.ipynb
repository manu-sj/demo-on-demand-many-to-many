{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ca8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f29af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging into hopsworks\n",
    "project = hopsworks.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401fb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching feature store\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81ffd7d",
   "metadata": {},
   "source": [
    "# Creating feature groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0021756f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing on-demand transformation functions\n",
    "from features.transaction_features import fetch_and_create_transactions_features\n",
    "from features.credict_card_features import fetch_and_create_credit_card_features\n",
    "from hopsworks.hsfs.feature import Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9f6e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature group\n",
    "transactions_fg = fs.get_or_create_feature_group(name=\"transactions_feature_group\", \n",
    "                                    version=1, \n",
    "                                    online_enabled=True,\n",
    "                                    primary_key=[\"cc_num\"],\n",
    "                                    features=[\n",
    "                                        Feature(name=\"tid\", type=\"string\"),\n",
    "                                    ],\n",
    "                                    event_time=\"transaction_time\",\n",
    "                                    transformation_functions=[\n",
    "                                        fetch_and_create_transactions_features(\"tid\").alias(\"cc_num\", \"category\", \"transaction_time\", \"amount\", \"transaction_city\", \"transaction_country\", \"fraud_label\") # Creating on-demand transformation by attaching transformation functions to Feature Group \n",
    "                                    ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3bccf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving feature group\n",
    "transactions_fg.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e32240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature group\n",
    "credit_cards_fg = fs.get_or_create_feature_group(name=\"credit_cards_feature_group\", \n",
    "                                    version=1, \n",
    "                                    online_enabled=True,\n",
    "                                    primary_key=[\"cc_num\"],\n",
    "                                    transformation_functions=[\n",
    "                                        fetch_and_create_credit_card_features(\"cc_num\", \"current_datetime\").alias(\"cc_num\", \"days_to_expiry\", \"age_at_transaction\", \"sex\", \"city\", \"country\") # Creating on-demand transformation by attaching transformation functions to Feature Group\n",
    "                                    ]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2af8005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving feature group\n",
    "credit_cards_fg.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec71254",
   "metadata": {},
   "source": [
    "# Creating a feature view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7c51f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features required for the feature view\n",
    "query = transactions_fg.select(['tid', 'cc_num', 'category', 'amount', 'transaction_city', 'transaction_country', 'fraud_label', 'transaction_time']).join(\n",
    "            credit_cards_fg.select(['days_to_expiry', 'age_at_transaction', 'sex', 'city','country']), prefix=\"fg2_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a80b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating feature view\n",
    "fv = fs.get_or_create_feature_view(name=\"fraud_feature_view\", \n",
    "                                   version=1, \n",
    "                                   query=query,\n",
    "                                   labels=[\"fraud_label\"],\n",
    "                                  logging_enabled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e7c851",
   "metadata": {},
   "source": [
    "# Fetch Saved model and save model to registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ee929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching model registry\n",
    "mr = project.get_model_registry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ada83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Python model in the model registry\n",
    "fraud_model = mr.python.create_model(\n",
    "    name=\"fraud_model\", \n",
    "    description=\"test description\", # Add a description for the model\n",
    "    input_example=[4467360740682089, \"51d90e9721e699f24382bf9dd10da420\"],     # Example input for testing deployments\n",
    "    feature_view=fv,            # Add a feature view to the model\n",
    "    training_dataset_version=1\n",
    ")\n",
    "\n",
    "# Save the pre-trained model to the specified model directory\n",
    "fraud_model.save('fraud_batch_model/xgb_classifier.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6492f56",
   "metadata": {},
   "source": [
    "# Deploy model \n",
    "\n",
    "Creating a predictor file that is used to make predictions in the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf88cd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile predict_example_new.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import hopsworks\n",
    "import joblib\n",
    "from xgboost import XGBClassifier\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "class Predict(object):\n",
    "\n",
    "    def __init__(self, project, model, async_logger):\n",
    "        \"\"\"Initializes the serving state, reads a trained model\"\"\"\n",
    "        fs = project.get_feature_store()\n",
    "\n",
    "        self.feature_view = fs.get_feature_view(name=\"fraud_feature_view\", version=1)\n",
    "\n",
    "        # Initialize feature logging for collecting transformed features.\n",
    "        self.feature_view.init_serving(feature_logger=async_logger)\n",
    "\n",
    "        self.hopsworks_model = model\n",
    "        self.model = joblib.load(os.environ[\"MODEL_FILES_PATH\"] + \"/xgb_classifier.pkl\")\n",
    "\n",
    "        print(\"Initialization Complete\")\n",
    "\n",
    "    def predict(self, inputs):\n",
    "        \"\"\"Serves a prediction request usign a trained model\"\"\"\n",
    "        cc_num = inputs[0][0]\n",
    "        tid = inputs[0][1]\n",
    "\n",
    "        # TODO : psycopg2 seems to get diconnected this should be moved into init if it was working properly\n",
    "        conn = psycopg2.connect(\n",
    "            database=\"test_db\",\n",
    "            user=\"admin\",\n",
    "            host=\"10.2.1.149\",\n",
    "            password=\"admin\",\n",
    "            port=5432,\n",
    "        )\n",
    "\n",
    "        feature_vector = self.feature_view.get_feature_vector(\n",
    "            {\"cc_num\": cc_num}, # Key to retrieve pre-computed features from the online feature store\n",
    "            passed_features={\"tid\": tid}, # Features to use \"as-is\" in the feature vector\n",
    "\n",
    "            request_parameters={ # Parameters to pass to the on-demand transfromations\n",
    "                \"cc_num\": cc_num,\n",
    "                \"current_datetime\": datetime.now(),\n",
    "            },\n",
    "            transformation_context={\"connection\": conn}, # Additional context to provide to the on-demand tranformations\n",
    "            return_type=\"pandas\"\n",
    "        )\n",
    "\n",
    "        # Drop the primary keys from the vector that is provided to the model\n",
    "        # The primary keys are neded to for feature logging.\n",
    "        parsed_feature_vector = feature_vector[\n",
    "            [\n",
    "                \"category\",\n",
    "                \"amount\",\n",
    "                \"transaction_city\",\n",
    "                \"transaction_country\",\n",
    "                \"fg2_days_to_expiry\",\n",
    "                \"fg2_age_at_transaction\",\n",
    "                \"fg2_sex\",\n",
    "                \"fg2_city\",\n",
    "                \"fg2_country\",\n",
    "            ]\n",
    "        ]\n",
    "\n",
    "        predictions = self.model.predict(\n",
    "            parsed_feature_vector\n",
    "        ).tolist()  # Numpy Arrays are not JSON serializable\n",
    "\n",
    "        # Logging the feature vector, predictions and the model. \n",
    "        # The same feature vector is provided as untransformed and transformed since there is no model-dependent transformation in this case.\n",
    "        self.feature_view.log(\n",
    "            untransformed_features=feature_vector.values.tolist(),\n",
    "            transformed_features=feature_vector.values.tolist(),\n",
    "            predictions=[predictions],\n",
    "            model=self.hopsworks_model,\n",
    "        )\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e97480",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Get the dataset API for the current project\n",
    "dataset_api = project.get_dataset_api()\n",
    "\n",
    "# Specify the local file path of the Python script to be uploaded\n",
    "local_script_path = \"predict_example_new.py\"\n",
    "\n",
    "# Upload the Python script to the \"Models\", and overwrite if it already exists\n",
    "uploaded_file_path = dataset_api.upload(local_script_path, \"Models\", overwrite=True)\n",
    "\n",
    "# Create the full path to the uploaded script for future reference\n",
    "predictor_script_path = os.path.join(\"/Projects\", project.name, uploaded_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864447e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the fraud model\n",
    "deployment = fraud_model.deploy(\n",
    "    name=\"fraudeployment\",  # Specify a name for the deployment\n",
    "    script_file=predictor_script_path,  # Provide the path to the Python script for prediction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafae278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the deployment\n",
    "deployment.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cf7608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the deployment.\n",
    "deployment.predict(inputs=[[4148299918528368, '201209bc29f918f28956da351d95ba37']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9ec611",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
